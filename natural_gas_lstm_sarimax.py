# -*- coding: utf-8 -*-
"""natural gas_LSTM_SARIMAX.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WlSsHJOvcujNw0IMSYMBuFm3KvHlmfFQ
"""

pip install probscale

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import pywt
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm  
from datetime import datetime
import math
import scipy as sp
import sklearn
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 150)
import time
from sklearn.metrics import mean_squared_error
from math import sqrt
import pandas_profiling as pp
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()
from scipy import stats
import numpy
import probscale
import seaborn
import math
from math import log
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from xgboost import XGBRegressor, XGBClassifier
import xgboost as xgb
import datetime
from pandas.plotting import lag_plot
from matplotlib import style
# %matplotlib inline
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

!pip install pyforest
from pyforest import *
import pandas_datareader as web
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 150)
plt.style.use('ggplot')

"""The objective of this investigation is to find a model, fit, and predict the natural gas spot price at the Henry Hub. Natural gas price is a random variable which follows a stochastic process with a random trend.

All the data, related to prices, consumptions, reserves, storages, productions,
pipelines etc., are collected from the U.S. Energy Information Administration’s website (EIA, US Energy Information Administration, 2016) which are accessible online for free. Climate data including cooling/heating degree days, extreme maximum/minimum temperature, mean temperature, and mean maximum/minimum temperature for New Orleans, LA, were downloaded from the National Centers for Environmental Information department of the National Oceanic and Atmospheric Administration (NOAA, 2016). All these data were combined together in a single file as an input data file.
"""

from google.colab import files
uploaded = files.upload()

data = pd.read_excel("Natutalgas.xlsx", parse_dates=True)
data.head()

data.shape

data = data.rename(columns=data.iloc[0]).drop(data.index[0])
data.head()

"""- A cooling degree day (CDD) is a measurement designed to quantify the demand for energy needed to cool buildings. It is the number of degrees that a day's average temperature is above 65 deg Fahrenheit (18 deg Celsius).
- A heating degree day (HDD) is a measurement designed to quantify the demand for energy needed to heat a building. It is the number of degrees that a day's average temperature is below 65o Fahrenheit (18o Celsius), which is the temperature below which buildings need to be heated.
"""

df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
print(df.head())

df = df.set_index(df.columns[0])
print(df.head())

df['hh_sp'].plot(figsize = (14,6), grid=True)
plt.title('Henry Hub Spot Price')
plt.ylabel('Price(Dollars per Million Btu)')
plt.show()

"""We are dealing with discrete parameter process of time series."""

# Plot
fig, axes = plt.subplots(1, 4, figsize=(10,3), sharex=True, sharey=True, dpi=100)
for i, ax in enumerate(axes.flatten()[:4]):
    lag_plot(df, lag=i+1, ax=ax)
    ax.set_title('Lag ' + str(i+1))

fig.suptitle('Lag Plots of Natural Gas \n(Points get wide and scattered with increasing lag -> lesser correlation)\n', y=1.15)    


plt.show()

lag_plot(df)
plt.show()

"""Running the example plots the data (t) on the x-axis against the data on the previous day (t-1) on the y-axis. We can see a large ball of observations along a diagonal line of the plot. It clearly shows a relationship or some correlation."""

from pandas.plotting import autocorrelation_plot
autocorrelation_plot(df.hh_sp)
plt.show()

"""- X — Lag: this is the years that are observed
- Y — Correlation: the correlation of the adjusted closed price according to time
- The dotted lines: as we can observe, the data lines are just above and below the first quartile, or within the 95% confidence interval. This will indicate the significance of the correlation. If the line is above or below the dotted line, not in between, we can say that the correlation is significant, and that the HH spot price is correlated to time. 

In this plot, we can see that in the 50 months, there is a correlation between spot price and time. Then, from year 60 to around 120 months we can again see some significant values.
"""

df.dtypes

df = df.astype('float')
df.info()

df['pct_chng'] = df['hh_sp'].pct_change() * 100
df.shape

plt.plot(df['pct_chng'])

"""First we will plot the auto-correlation of the spot price with the previous lags and see if there is any significant correlation that the returns have with the previous values. So, we will review the correlation between the time series of interest in lagged versions of itself. """

plt.figure()
plt.subplot(211)
plot_acf(df['hh_sp'], ax=plt.gca())
plt.subplot(212)
plot_pacf(df['hh_sp'], ax=plt.gca())
plt.xlabel('Lag')
plt.tight_layout()
plt.show()

"""The blue shaded area is the 95% confidence interval. We can see that several lags till 10 months. """

np.isnan(df1).sum()

def fill_missing(df):
  for row in range(df.shape[0]):
    for col in range(df.shape[1]):
      if np.isnan(df[row,col]):
        df[row,col]= df[row-1, col]

fill_missing(df.values)
np.isnan(df).sum()

fill_missing(df.values)
np.isnan(df).sum()

pp.ProfileReport(df)

df.shape

"""We could take advantage of neural networks because of their properties, which
help us to build a strong and robust model. The ability to learn and generalization are two important features of neural networks. Neural networks are able to solve complex problems by dividing them into simple tasks. An artificial neural networks consist of activation function that could be linear or nonlinear. By choosing the various nonlinear functions and different arrangement of the neurons and layers, nonlinear response to the stimuli would be captured. Nonlinearity handling is one of the most useful features of the neural networks which helps us to model complex functions (Haykin, 1999).
In neural network design, the networks learn to decrease the difference between
the response and actual answers. There is an input-output mapping between the inputs and responses based on model-free estimation. After training the neural networks and defining the synaptic weight parameters, there is a single response for each specific inputs.

ANNs are generally described as a set of interconnected neurons grouped in layers which transfer signals between each other. The connections have numerical weights that can be adjusted based on feedback or comparison to a reference data set that make it adaptable to inputs and capable of learning.
Layers are key concepts in neural network architecture which, are made of a set
of interconnected node named neurons containing an output function named activation function. The linkage between neurons are called synapsis which contained numerical weights. Independent variables as input data enter into the network via first layer called input layer, and dependent variables as results are obtained from the last layer named output layer.

Each node of a layer is connected to the all nodes of the next layer. Each input to a layer is multiplied by a weight, then will be combined together with reference to a threshold and activation function and use them to determine the layer’s outputs. The output could be a final result or an input to the next layer. The weights are initialized by random numbers and then trained by a large number of available data to get an accurate response.

Neural networks are often applied to solve the problems with unknown correlation
or cumbersome non-linear models which cannot be defined easily. As the exact solution or even the form of the solution is vague, neural networks require a large number of runs to determine the best solution. A neural network after training with defined weights is ready to use as a powerful analytical tool for other data, plugging in inputs and readily get the results at the output layer.

We will be using Multilayer neural network. This network has 5 nodes as input which feed into the second layer or first hidden layer as input values. The outputs from the first hidden layers go to the second hidden layer as input. The last layer is the output layer which has a single output as a final result.

The models for prediction the price are classified into two different categories as
univariate and multivariate models. Univariate models just attempt to simulate the phenomena by the target variables itself and its lags. In this models, other parameters that may affect the behavior of the dependent variables are not considered assuming all the causes are reflected in the prices. The second class, multivariate models, enters the different parameters, which are called exogenous variables. These variables may affect the dependent variable behavior based on the economic, social, or political situations. The problem with these models is to require forecasting these parameters in the future in order to predict the target variable. 

Some factors may also affect the supply and demand of the natural gas such as
macroeconomic parameters, natural gas production and consumption, storage and
inventories, weather conditions, pipeline capacity, and fuel substitutions. Political issues, military interface, and economic instability as an important factor in the energy market, particularly in the oil market are not considered in the previous researches because evaluation and quantizing of these variables is not an easy task.
"""

import seaborn as sns
sns.despine(left=True)

# Plot a simple histogram with binsize determined automatically
sns.distplot(df['hh_sp'], color="b")
plt.title('Henry hub spot price')
plt.grid(True)
plt.show()

"""]The first step in statistical analysis is to investigate the time series distribution.
The Jarque-Bera normality test shows that the price time series is not normally distributed but it follows a log-normal distribution (Jarque & Bera, 1987). Below fig illustrates the distribution of the logarithmic price with respect to the reference red line in the normal plot. Therefore, the log price or the return of the natural gas price is normally distributed and follows the Gaussian distribution. Now, it is appropriate to assume normal
distribution for first the difference of variable and/or the logarithm of the gas price. 
"""

from scipy import stats
stats.jarque_bera(df['hh_sp'])

fig, ax = plt.subplots(figsize=(10, 6))
plt.grid(True)
fig = probscale.probplot(df['hh_sp'], ax=ax, plottype='pp', bestfit=True,
                         problabel='Percentile', datalabel='HenryHub spot price (S/MMBtu)',
                         scatter_kws=dict(label='HH original data'),
                         line_kws=dict(label='Best-fit line'))
ax.legend(loc='upper left')
seaborn.despine()

fig, ax = plt.subplots(figsize=(10, 6))
plt.grid(True)
fig = probscale.probplot(df['hh_sp'], ax=ax, plottype='pp', bestfit=True,
                         datascale='log',
                         problabel='Percentile', 
                         datalabel='Log of HenryHub price ($/MMBtu)',
                         scatter_kws=dict(label='HH lognormal data'),
                         line_kws=dict(label='Best-fit line'))
ax.legend(loc='upper left')
seaborn.despine()

df.index

df.columns

"""We will use a metric (variance inflation factor) through which we will identify how much a variable is contributiing to the std error in a regression. So, we will detect multicolinierity in our dataset by doing so. Large VIF means significant multicolinearty exists."""

X1 = sm.tools.add_constant(df)
series = pd.Series([variance_inflation_factor(X1.values, i) for i in range(X1.shape[1])], 
                   index = X1.columns)
print('Variance Inflation Factor')
print('_' *100)
display(series)

"""We have autocorrelation in excess of 0.5 beyond. We can theoretically use one of the high autocorrelation lags to develop an LSTM model.

if there is autocorrelation the correlation is linear ( not non-linear ) because common autocorrelation tests for linear correlation. Any LSTM is able to capture this linear correlations by default, it does not matter how many linear correlations are in the time series, the LSTM will capture it.

LSTMs are quite useful in time series prediction tasks involving autocorrelation, the presence of correlation between the time series and lagged versions of itself, because of their ability to maintain state and recognize patterns over the length of the time series. The recurrent architecture enables the states to persist, or communicate between updates of the weights as each epoch progresses. Further, the LSTM cell architecture enhances the RNN by enabling long term persistence in addition to short term, which is fascinating!
"""

train_data = df[df.index < '2015'].copy() # spliting tarining set
test_data = df[df.index >= '2015'].copy() # spliting validation set
print(train_data.shape, test_data.shape)

X, y = train_data, test_data

# Feature Scaling Normalization
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X = scaler.fit_transform(X)
print(X)
print('\n')
print(X.shape)

# shaping data from neural network
X_train = []
y_train = []
for i in range(1, X.shape[0]):
  X_train.append(X[i-1:i])
  y_train.append(X[i,0])
  if i <= 2:
    print(X_train)
    print('\n')
    print(y_train)
    print()

X_train, y_train = np.array(X_train), np.array(y_train)
print(X_train.shape, y_train.shape)

from tensorflow.keras import layers
from tensorflow.keras.layers import Dense, Dropout, LSTM
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
tf.enable_eager_execution ()

from __future__ import absolute_import, division, print_function, unicode_literals

try:
  import tensorflow.compat.v2 as tf
except Exception:
  pass

tf.enable_v2_behavior()

print(tf.__version__)

!tf_upgrade_v2 -h

regressor = tf.keras.Sequential()
regressor.add(tf.keras.layers.GRU(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 49)))
regressor.add(tf.keras.layers.Dropout(0.2))

regressor.add(tf.keras.layers.GRU(units = 50, return_sequences = True))
regressor.add(tf.keras.layers.Dropout(0.2))

regressor.add(tf.keras.layers.GRU(units = 50, return_sequences = True))
regressor.add(tf.keras.layers.Dropout(0.2))

regressor.add(tf.keras.layers.GRU(units = 50, return_sequences = False))
regressor.add(tf.keras.layers.Dropout(0.2))
regressor.add(tf.keras.layers.Dense(units = 25))
regressor.add(tf.keras.layers.Dense(units = 1))

regressor.summary()

look_back = train_data.tail(1)
data = look_back.append(test_data)
print(data)

inputs = scaler.transform(data)
inputs

# shaping data from neural network
X_test = []
y_test = []
for i in range(1, inputs.shape[0]):
  X_test.append(inputs[i-1:i])
  y_test.append(inputs[i,0])

X_test, y_test= np.array(X_test), np.array(y_test)
print(X_test.shape, y_test.shape)

regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')
early_stopping = EarlyStopping(monitor='loss', patience=10)
history = regressor.fit(X_train, y_train, 
                        epochs = 200, 
                        batch_size = 32, validation_data = (X_test, y_test),
                        callbacks=[early_stopping])

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model train vs validation loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='best')
plt.show()

y_pred = regressor.predict(X_test)

scaler.scale_

scale = 1/8.54700855e-02
scale

y_pred = y_pred * scale
y_pred = pd.DataFrame(y_pred)
y_pred.rename(columns={0: "Prediction"}, inplace=True)
y_pred

y_test = y_test * scale
y_test = pd.DataFrame(y_test)
y_test.rename(columns={0: "Actual"}, inplace=True)
y_test

combine = pd.concat([y_test, y_pred], axis=1)
combine

mean_test = y_test.mean()
mean_test

mean_pred = y_pred.mean()
mean_pred

from sklearn.metrics import r2_score, mean_absolute_error 
mse = mean_squared_error(y_test, y_pred)
print('mse:', mse)
rmse = np.sqrt(mse)
print('rmse:', rmse)
mae = mean_absolute_error(y_test, y_pred)
print('mae:', mae)

plt.figure(figsize = (10,6))
plt.plot(y_test, color = 'red', label = 'Actual HenryHub spot price')
plt.plot(y_pred, color = 'blue', label = 'Predicted price')
plt.title ('HH Spot Price prediction')
plt.xlabel('Time')
plt.ylabel('Price ($/MMBtu)')
plt.legend()
plt.show()

df['pred_price'] = np.where(df['hh_sp'].shift(-1) > df['hh_sp'], 1, 0)
y = df['pred_price']
x = df.drop(columns = ['pred_price'])

reg = XGBRegressor()
reg.fit(x,y)

from xgboost import plot_importance
plt.rcParams["figure.figsize"] = (10, 15)
plot_importance(reg)
plt.show()

model = sm.OLS(df.hh_sp, df.im_pr +	df.ex_pr+	df.tot_prod+
               df.prod_gw+	df.prod_ow+	df.prod_sg	+df.prod_cw+
               df.prod_mp+	df.prod_liq+	df.prod_dng+	df.imp_tot+	df.imp_pip	+
               df.ImpLiq+	df.exp_tot+	df.exp_pip+	df.exp_liq+	df.stor_cap+
               df.stor_vol+	df.stor_inj+	df.stor_wd+	df.stor_wd_net+	df.tot_cnsm+	
               df.cnsm_pf+	df.cnsm_dist+	df.del_con+	df.cd_ny	+df.hd_ny+	df.ex_max_nyt+
               df.ex_min_nyt+	df.mean_max_nyt+	df.mean_min_nyt+	df.mean_nyt+	df.cd_tx	+df.hd_tx+
               df.ex_max_txt+	df.ex_min_txt+	df.mean_max_txt+	df.mean_min_txt+	df.mean_txt+	df.cd_la+	df.hd_la+
               df.ex_max_lat+	df.ex_min_lat+	df.mean_max_lat+	df.mean_min_lat+	df.mean_lat+	df.wti_sp)

results = model.fit()
print(results.summary())

from sklearn import linear_model
clf = linear_model.LinearRegression()
clf.fit(df)

"""## **ARIMA**"""

import itertools
import time

df = data.copy()
df.set_index('Date', inplace=True)
df.tail(2)

df.info()

import seaborn as sns
sns.despine(left=True)

# Plot a simple histogram with binsize determined automatically
sns.distplot(df['hh_sp'], color="b")
plt.title('Henry hub spot price')
plt.grid(True)
plt.show()

fig, ax = plt.subplots(figsize=(10, 6))
plt.grid(True)
fig = probscale.probplot(df['hh_sp'], ax=ax, plottype='pp', bestfit=True,
                         problabel='Percentile', datalabel='HenryHub spot price (S/MMBtu)',
                         scatter_kws=dict(label='HH original data'),
                         line_kws=dict(label='Best-fit line'))
ax.legend(loc='upper left')
seaborn.despine()

df = df.astype(float)
df.info()

res = sm.tsa.seasonal_decompose(df['hh_sp'],freq=12)
fig = res.plot()
fig.set_figheight(8)
fig.set_figwidth(15)
plt.show()

#ADF-test(Original-time-series)
res = sm.tsa.adfuller(df['hh_sp'],regression='nc')
print('p-value:{}'.format(res[1]))

#ADF-test(differenced-time-series)
res = sm.tsa.adfuller(df['hh_sp'].diff().dropna(),regression='nc')
#’nc’ : no constant, no trend
print('p-value:{}'.format(res[1]))

"""It's important to choose carefully a period of the data which will be used in predicting. Because, The results depend on the period."""

# To install the library 
!pip install pmdarima 
  
# Import the library 
from pmdarima import auto_arima

df.isnull().sum()

def fill_missing(df):
  for row in range(df.shape[0]):
    for col in range(df.shape[1]):
      if np.isnan(df[row,col]):
        df[row,col]= df[row-1, col]

fill_missing(df.values)
np.isnan(df).sum()

fill_missing(df.values)
np.isnan(df).sum()

df.shape

"""### Recursive Feature Elimination:

A popular feature selection method within sklearn is the Recursive Feature Elimination.  RFE selects features by considering a smaller and smaller set of regressors.  The starting point is the original set of regressors. Less important regressors are recursively pruned from the initial set.  The procedure is repeated until a desired set of features remain.  That number can either be a priori specified, or can be found using cross validation. In fact, RFE offers a variant – RFECV – designed to optimally find the best subset of regressors.  
"""

# linear regression feature importance
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from sklearn.feature_selection import RFECV

y = df['hh_sp']
X = df.drop(['hh_sp'], axis=1)
names=pd.DataFrame(X.columns)

#Feature ranking with recursive feature elimination and cross-validated selection of the best number of features

#use linear regression as the model
lin_reg = LinearRegression()

#This is to select 8 variables: can be changed and checked in model for accuracy
mod =  RFECV(lin_reg, step=1, cv=20) #RFE(lin_reg, 4, step=1)

mod_fit = mod.fit(X,y) #to fit

#The feature ranking, such that ranking_[i] corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.

rankings=pd.DataFrame(mod_fit.ranking_, index=names) #Make it into data frame
rankings.rename(columns ={0: 'Rank'}, inplace=True)
rankings.transpose()

"""14 features are selected for inclusion in the model"""

rankings=pd.DataFrame(mod_fit.ranking_)

#Concat and name columns
ranked=pd.concat([names,rankings], axis=1)
ranked.columns = ["Feature", "Rank"]

#Select most important (Only 1's)
imp_feat = ranked.loc[ranked['Rank'] ==1] 
imp_feat.transpose()

imp_feat['Rank'].count()

df.shape

# creating target variable
df['target'] = df['hh_sp'].shift(-5) 
df.dropna(inplace=True)
n = df.drop(['target'], axis=1) # storing in a dataframe for future use

dataset = df[['im_pr','ex_pr','prod_mp','prod_liq','prod_dng','imp_tot','imp_pip','ImpLiq',
              'mean_max_txt', 'mean_min_txt', 'mean_txt', 'mean_max_lat', 'mean_min_lat','mean_lat', 'target' ]]
dataset.describe()

from sklearn.model_selection import cross_val_score, KFold, cross_validate, train_test_split, TimeSeriesSplit


X = np.array(dataset.drop(['target'], 1))
y = np.array(dataset['target'])

tscv = TimeSeriesSplit(max_train_size=None, n_splits=5)
for train_samples, test_samples in tscv.split(X):
  #print("TRAIN:", train_samples, "TEST:", test_samples)
  X_train, X_test = X[train_samples], X[test_samples]
  y_train, y_test = y[train_samples], y[test_samples]

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

feature_names = ['im_pr','ex_pr','prod_mp','prod_liq','prod_dng','imp_tot','imp_pip','ImpLiq',
                 'mean_max_tx', 'mean_min_txt', 'mean_txt', 'mean_max_lat', 'mean_min_lat','mean_lat' ]

X_train = pd.DataFrame(data=X_train, columns=feature_names)
X_train.index = dataset[:186].index
y_train = pd.DataFrame(y_train, columns = ['target'])
y_train.index = X_train.index

X_test = pd.DataFrame(data=X_test, columns=feature_names)
X_test.index = dataset[186:].index
y_test = pd.DataFrame(y_test, columns = ['target'])
y_test.index = X_test.index

X_train.shape

#X_train_tran = np.log(X_train).diff()
#X_train_tran = X_train_tran.copy()
#X_train_tran = X_train_tran[~X_train_tran.isin([np.nan, np.inf, -np.inf]).any(1)]

X_train_tran = np.log(X_train/X_train.shift(1)).dropna()
X_train_tran.describe()

y_train_tran = np.log(y_train/y_train.shift(1)).dropna()
y_train_tran.describe()

#y_train_tran = np.log(y_train).diff()
#y_train_tran = y_train_tran.copy()
#y_train_tran = y_train_tran[~y_train_tran.isin([np.nan, np.inf, -np.inf]).any(1)]
#y_train_tran.describe()

from statsmodels.tsa.stattools import adfuller
X = y_train_tran['target'].values
result = adfuller(X)
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
	print('\t%s: %.3f' % (key, value))

X_train_tran.plot(figsize=(14,6))
plt.show()

X_train_tran.hist(figsize=(14,6))
plt.tight_layout()
plt.show()

!pip install pyramid-arima
from pyramid.arima import auto_arima

# Fit auto_arima function 
stepwise_fit = auto_arima(y_train_tran, X_train_tran, start_p = 1, start_q = 1, 
                          max_p = 7, max_q = 7, m = 12, 
                          start_P = 0, seasonal = True, 
                          d = None, D = 1, trace = True, 
                          error_action ='ignore',   # we don't want to know if an order does not work 
                          suppress_warnings = True,  # we don't want convergence warnings 
                          stepwise = True)           # set to stepwise 
  
# To print the summary 
stepwise_fit.summary()

from statsmodels.tsa.statespace.sarimax import SARIMAX

# fit model
model = sm.tsa.statespace.SARIMAX(endog=y_train_tran, exog=X_train_tran, order=(0, 0, 0), 
                seasonal_order=(2, 1, 1, 12),
                enforce_invertibility=False, 
                enforce_stationarity=False)
model_fit = model.fit(disp=False)
model_fit.summary()

model_fit.plot_diagnostics(figsize=(14, 6))
plt.tight_layout()
plt.show()

# Fit a local level model
endog=ytest_tran
local_model = sm.tsa.UnobservedComponents(endog, 'local level')
# mod_local is an instance of the UnobservedComponents class

# Fit the model via maximum likelihood
result = local_model.fit()
# Note that res_ll is an instance of the UnobservedComponentsResults class

# Show the summary of results
print(result.summary())

# Show a plot of the estimated level and trend component series
fig = result.plot_components()
plt.tight_layout()

X_test_tran = np.log(X_test).diff()
Xtest_tran = X_test_tran.copy()
Xtest_tran = Xtest_tran[~Xtest_tran.isin([np.nan, np.inf, -np.inf]).any(1)]
Xtest_tran.describe()

y_test_log = np.log(y_test['target']).dropna()
y_test_tran = y_test_log.diff().dropna()
ytest_tran = y_test_tran.copy()
#ytest_tran = ytest_tran[~ytest_tran.isin([np.nan, np.inf, -np.inf]).any(1)]
ytest_tran.describe()

pred = model_fit.predict(start=1,end=36, exog = Xtest_tran)

from sklearn.metrics import mean_squared_error, mean_absolute_error
print('SARIMAX model MSE:{}'.format(mean_squared_error(ytest_tran,pred)))
print('SARIMAX model MAE: {}'.format(mean_absolute_error(ytest_tran,pred)))

Error = np.sum(np.abs(np.subtract(ytest_tran,pred)))
Average = np.sum(ytest_tran)
MAPE = Error/Average

MAPE

mape = (np.abs(pred.mean() - ytest_tran.mean())/np.abs(ytest_tran.mean())) 
mape

pred = pred.tail()
pred.index = df.tail().index
pred = pd.DataFrame(pred)
pred.rename(columns ={0: 'Predicted'}, inplace=True)
pred

y_test_log[-1]

y_test_log[36]

rebuilt = pred.cumsum().fillna(0)
print('\nCumsum:\n', rebuilt.tail())
rebuilt_further = rebuilt + y_test_log[-1]
print('\nDe-difference:\n',rebuilt_further.tail())
original = np.exp(rebuilt_further)
print('\nOriginal form:\n',original.tail())

original

d = df.tail()
d.reset_index(inplace=True)
d = d.append(pd.DataFrame({'Date': pd.date_range(start=d.Date.iloc[-1], periods=6,
                                             freq='m', closed='right')}))
d.set_index('Date', inplace=True)
d = d.tail()
original.index = d.index
original

# Graph
fig, ax = plt.subplots(figsize=(10,5))
ax.set(title='Henry Hub Spot price', xlabel='Date', ylabel='USD / Mbtu')
# Plot data points
y_test.plot(ax=ax, label='Observed')
# Plot predictions
original.plot(ax=ax, style='r--', label='One-step-ahead forecast')
legend = ax.legend(loc='best')

# In-sample one-step-ahead predictions
predict = model_fit.get_prediction()
predict_ci = predict.conf_int()

predict_ci

y_train

# Dynamic predictions
predict_dy = model_fit.get_prediction(dynamic=36)
predict_dy_ci = predict_dy.conf_int()

combine = pd.concat([original.tail(), y_test.tail()], axis=1)
combine = combine.round(decimals=2)
combine['accuracy'] = round(combine.apply(lambda row: row.Predicted / 
                                            row.target *100, axis = 1),2)
combine['accuracy'] = pd.Series(["{0:.2f}%".format(val) for val in combine['accuracy']], 
                                     index = combine.index)
combine = combine.reset_index()
combine

fig = go.Figure()
fig.add_trace(go.Scatter(x=combine['Date'],y=combine['target'],
                         name="Actual spot price ($/mbtu"))

fig.add_trace(go.Scatter(x=combine['Date'],y=combine['Predicted'],
                         name="Predicted spot price ($/mbtu"))

fig.update_layout(title="Henry Hub Actual vs Predicted Spot Price",
   yaxis_title="Price ($/Mbtu)",
    font=dict(family="Courier New, monospace",size=18,color="#7f7f7f"))
fig.update_layout(autosize=False,width=800,height=400,)
fig.update_layout(legend_orientation="h")
fig.show()

# In-sample one-step-ahead predictions
predict = model_fit.get_prediction(start=1,end=37, endog = y_test)
predict_ci = predict.conf_int()

print(predict)

# In-sample one-step-ahead predictions
predict = model_fit.get_prediction(y= y_test,dynamic=False)
predict_ci = predict.conf_int()

ax = y['2012-07-15':].plot(label='observed')
predict.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 4))
ci = predict_ci.loc['2015-03-15':]
ax.fill_between(ci.index,
                ci.iloc[:, 0],
                ci.iloc[:, 1], color='k', alpha=.2)
ax.set_xlabel('Date')
ax.set_ylabel('Henry Hub Spot price (Monthly), $/mbtu')
plt.legend()
plt.show()

# Graph
fig, ax = plt.subplots(figsize=(14,6))
npre = 4
ax.set(title='Henry Hub Spot price', xlabel='Date', ylabel='USD / Mbtu')

# Plot data points
dataset.loc['2012-07-31':, 'target'].plot(ax=ax, style='o', label='Observed')

# Plot predictions
predict.predicted_mean.loc['2012-07-31':].plot(ax=ax, style='r--', label='One-step-ahead forecast')
ci = predict_ci.loc['2012-07-31':]
ax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)
predict_dy.predicted_mean.loc['2012-07-31':].plot(ax=ax, style='g', label='Dynamic forecast (2012)')
ci = predict_dy_ci.loc['2012-07-31':]
ax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='g', alpha=0.1)

legend = ax.legend(loc='lower right')